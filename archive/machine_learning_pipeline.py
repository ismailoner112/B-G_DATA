import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import pickle
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# ML kÃ¼tÃ¼phaneleri
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, roc_curve, accuracy_score,
                           precision_score, recall_score, f1_score)

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

class MobileSecurityMLPipeline:
    def __init__(self, workspace_path='.'):
        self.workspace_path = Path(workspace_path)
        self.normal_dir = self.workspace_path / 'normal_apks_1mobile' / 'normal_apks_1mobile'
        self.malware_dir = self.workspace_path / 'malware_apks_1mobile' / 'malware_apks_1mobile'
        self.results_dir = self.workspace_path / 'ml_results'
        self.results_dir.mkdir(exist_ok=True)
        
        # Modeller ve sonuÃ§lar
        self.models = {}
        self.results = {}
        self.feature_data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        
    def extract_features_from_file(self, file_path, label, max_rows=10000):
        """Tek dosyadan Ã¶zellik Ã§Ä±karÄ±mÄ±"""
        try:
            # DosyayÄ± oku (bellek tasarrufu iÃ§in sÄ±nÄ±rlÄ± satÄ±r)
            df = pd.read_csv(file_path, nrows=max_rows)
            
            features = {
                'file_name': file_path.name,
                'label': label,
                'file_size_mb': file_path.stat().st_size / (1024 * 1024)
            }
            
            # Temel istatistikler
            features['total_rows'] = len(df)
            features['total_columns'] = len(df.columns)
            
            # Sistem Ã§aÄŸrÄ±sÄ± Ã¶zellikleri
            if 'syscall' in df.columns:
                syscalls = df['syscall'].dropna()
                syscall_counts = Counter(syscalls)
                
                # En yaygÄ±n sistem Ã§aÄŸrÄ±larÄ±
                top_syscalls = ['read', 'write', 'ioctl', 'recvfrom', 'sendto', 
                              'futex', 'epoll_pwait', 'rt_sigprocmask', 'getuid', 'fstat']
                
                for syscall in top_syscalls:
                    # Tab karakterini temizle
                    syscall_clean = syscall + '\t'
                    features[f'syscall_{syscall}_count'] = syscall_counts.get(syscall_clean, 0)
                
                features['unique_syscalls'] = len(syscall_counts)
                features['total_syscalls'] = len(syscalls)
                features['syscall_entropy'] = self.calculate_entropy(list(syscall_counts.values()))
            
            # Process Ã¶zellikleri
            if 'processName' in df.columns:
                processes = df['processName'].dropna()
                features['unique_processes'] = len(processes.unique())
                features['total_processes'] = len(processes)
                
                # En yaygÄ±n process'ler
                common_processes = ['RenderThread', 'Chrome_IOThread', 'Binder', 'pool']
                process_counts = Counter(processes)
                for proc in common_processes:
                    features[f'process_{proc}_count'] = sum(count for name, count in process_counts.items() 
                                                          if proc.lower() in name.lower())
            
            # Zamansal Ã¶zellikler
            if 'timestamp' in df.columns:
                timestamps = pd.to_numeric(df['timestamp'], errors='coerce').dropna()
                if len(timestamps) > 1:
                    time_diffs = np.diff(timestamps)
                    features['avg_time_diff'] = np.mean(time_diffs)
                    features['std_time_diff'] = np.std(time_diffs)
                    features['max_time_diff'] = np.max(time_diffs)
                    features['min_time_diff'] = np.min(time_diffs)
            
            # AÄŸ aktivitesi Ã¶zellikleri
            network_columns = ['sin6_addr', 'sin6_port', 'sa_family']
            for col in network_columns:
                if col in df.columns:
                    features[f'{col}_non_null_count'] = df[col].notna().sum()
            
            # ArgÃ¼man analizi
            arg_columns = [col for col in df.columns if col.startswith('value')]
            if arg_columns:
                features['avg_args_per_call'] = df[arg_columns].notna().sum(axis=1).mean()
                features['max_args_per_call'] = df[arg_columns].notna().sum(axis=1).max()
            
            return features
            
        except Exception as e:
            print(f"   Hata: {file_path.name} - {str(e)}")
            return None
    
    def calculate_entropy(self, values):
        """Shannon entropisi hesapla"""
        if not values:
            return 0
        
        total = sum(values)
        probabilities = [v/total for v in values if v > 0]
        entropy = -sum(p * np.log2(p) for p in probabilities)
        return entropy
    
    def extract_all_features(self, sample_size_per_class=500):
        """TÃ¼m dosyalardan Ã¶zellik Ã§Ä±karÄ±mÄ±"""
        print("ğŸ”§ Ã–zellik Ã§Ä±karÄ±mÄ± baÅŸlatÄ±lÄ±yor...")
        
        # Dosya listelerini al
        normal_files = list(self.normal_dir.glob('*.csv'))[:sample_size_per_class]
        malware_files = list(self.malware_dir.glob('*.csv'))[:sample_size_per_class]
        
        print(f"   Normal APK dosyalarÄ±: {len(normal_files)}")
        print(f"   ZararlÄ± APK dosyalarÄ±: {len(malware_files)}")
        
        all_features = []
        
        # Normal dosyalarÄ± iÅŸle
        print("    Normal APK dosyalarÄ± iÅŸleniyor...")
        for i, file in enumerate(normal_files):
            if i % 50 == 0:
                print(f"      Progress: {i}/{len(normal_files)}")
            features = self.extract_features_from_file(file, 0)  # 0 = Normal
            if features:
                all_features.append(features)
        
        # Malware dosyalarÄ± iÅŸle
        print("    ZararlÄ± APK dosyalarÄ± iÅŸleniyor...")
        for i, file in enumerate(malware_files):
            if i % 50 == 0:
                print(f"      Progress: {i}/{len(malware_files)}")
            features = self.extract_features_from_file(file, 1)  # 1 = Malware
            if features:
                all_features.append(features)
        
        # DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
        self.feature_data = pd.DataFrame(all_features)
        
        # Eksik deÄŸerleri doldur
        numeric_columns = self.feature_data.select_dtypes(include=[np.number]).columns
        self.feature_data[numeric_columns] = self.feature_data[numeric_columns].fillna(0)
        
        print(f"Ã–zellik Ã§Ä±karÄ±mÄ± tamamlandÄ±! Toplam Ã¶rnek: {len(self.feature_data)}")
        print(f"Toplam Ã¶zellik sayÄ±sÄ±: {len(self.feature_data.columns) - 2}")  # file_name ve label hariÃ§
        
        # Ã–zellikleri kaydet
        self.feature_data.to_csv(self.results_dir / 'extracted_features.csv', index=False)
        
        return self.feature_data
    
    def prepare_data_for_training(self):
        """EÄŸitim iÃ§in veriyi hazÄ±rla"""
        print(" Veri eÄŸitim iÃ§in hazÄ±rlanÄ±yor...")
        
        if self.feature_data is None:
            raise ValueError("Ã–nce Ã¶zellik Ã§Ä±karÄ±mÄ± yapÄ±lmalÄ±!")
        
        # Ã–zellik ve hedef deÄŸiÅŸkenleri ayÄ±r
        feature_columns = [col for col in self.feature_data.columns 
                          if col not in ['file_name', 'label']]
        
        X = self.feature_data[feature_columns]
        y = self.feature_data['label']
        
        # Train-test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Ã–zellik Ã¶lÃ§eklendirme
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"EÄŸitim seti: {len(self.X_train)} Ã¶rnek")
        print(f"Test seti: {len(self.X_test)} Ã¶rnek")
        print(f"Ã–zellik sayÄ±sÄ±: {self.X_train.shape[1]}")
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        train_dist = pd.Series(self.y_train).value_counts()
        test_dist = pd.Series(self.y_test).value_counts()
        
        print("\n SÄ±nÄ±f DaÄŸÄ±lÄ±mlarÄ±:")
        print(f"   EÄŸitim - Normal: {train_dist.get(0, 0)}, ZararlÄ±: {train_dist.get(1, 0)}")
        print(f"   Test - Normal: {test_dist.get(0, 0)}, ZararlÄ±: {test_dist.get(1, 0)}")
    
    def train_models(self):
        """Ã‡oklu model eÄŸitimi"""
        print("Model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
        
        # Model tanÄ±mlarÄ±
        models_config = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100, 
                random_state=42, 
                n_jobs=-1,
                max_depth=10
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                random_state=42,
                max_depth=6
            ),
            'SVM': SVC(
                probability=True,
                random_state=42,
                kernel='rbf'
            ),
            'Neural Network': MLPClassifier(
                hidden_layer_sizes=(100, 50),
                random_state=42,
                max_iter=500
            )
        }
        
        # Her modeli eÄŸit
        for name, model in models_config.items():
            print(f"   ğŸ‹ï¸ {name} eÄŸitiliyor...")
            
            # Veri seÃ§imi (SVM ve NN iÃ§in scaled, diÄŸerleri iÃ§in original)
            if name in ['SVM', 'Neural Network']:
                X_train_use = self.X_train_scaled
                X_test_use = self.X_test_scaled
            else:
                X_train_use = self.X_train
                X_test_use = self.X_test
            
            # Modeli eÄŸit
            model.fit(X_train_use, self.y_train)
            
            # Tahminler
            y_pred = model.predict(X_test_use)
            y_pred_proba = model.predict_proba(X_test_use)[:, 1]
            
            # Metrikleri hesapla
            accuracy = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred)
            recall = recall_score(self.y_test, y_pred)
            f1 = f1_score(self.y_test, y_pred)
            auc = roc_auc_score(self.y_test, y_pred_proba)
            
            # Cross-validation
            if name in ['SVM', 'Neural Network']:
                cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5)
            else:
                cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5)
            
            # SonuÃ§larÄ± kaydet
            self.models[name] = model
            self.results[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'auc_score': auc,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba,
                'confusion_matrix': confusion_matrix(self.y_test, y_pred)
            }
            
            print(f"      Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, CV: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
        
        print(" TÃ¼m modeller eÄŸitildi!")
    
    def create_ml_visualizations(self):
        """ML sonuÃ§larÄ±nÄ±n gÃ¶rselleÅŸtirilmesi"""
        print("ML gÃ¶rselleÅŸtirmeleri oluÅŸturuluyor...")
        
        # 1. Model performans karÅŸÄ±laÅŸtÄ±rmasÄ±
        self.plot_model_comparison()
        
        # 2. Confusion matrix'ler
        self.plot_confusion_matrices()
        
        # 3. ROC eÄŸrileri
        self.plot_roc_curves()
        
        # 4. Feature importance (Random Forest iÃ§in)
        self.plot_feature_importance()
        
        # 5. SÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±
        self.plot_class_distributions()
        
        print("TÃ¼m ML gÃ¶rselleÅŸtirmeleri oluÅŸturuldu!")
    
    def plot_model_comparison(self):
        """Model performans karÅŸÄ±laÅŸtÄ±rmasÄ±"""
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_score']
        model_names = list(self.results.keys())
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            values = [self.results[model][metric] for model in model_names]
            colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']
            
            bars = axes[i].bar(model_names, values, color=colors[:len(model_names)])
            axes[i].set_title(f'{metric.upper()} KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
            axes[i].set_ylabel(metric.upper())
            axes[i].set_ylim(0, 1)
            
            # DeÄŸerleri bar Ã¼zerine ekle
            for bar, value in zip(bars, values):
                axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
            
            axes[i].tick_params(axis='x', rotation=45)
        
        # Cross-validation skorlarÄ±
        cv_means = [self.results[model]['cv_mean'] for model in model_names]
        cv_stds = [self.results[model]['cv_std'] for model in model_names]
        
        axes[5].bar(model_names, cv_means, yerr=cv_stds, capsize=5, 
                   color=['skyblue', 'lightgreen', 'lightcoral', 'gold'][:len(model_names)])
        axes[5].set_title('Cross-Validation SkorlarÄ±', fontweight='bold')
        axes[5].set_ylabel('CV Score')
        axes[5].set_ylim(0, 1)
        axes[5].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_confusion_matrices(self):
        """Confusion matrix gÃ¶rselleÅŸtirmeleri"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for i, (model_name, results) in enumerate(self.results.items()):
            cm = results['confusion_matrix']
            
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Normal', 'ZararlÄ±'],
                       yticklabels=['Normal', 'ZararlÄ±'],
                       ax=axes[i])
            
            axes[i].set_title(f'{model_name} - Confusion Matrix', fontweight='bold')
            axes[i].set_xlabel('Tahmin Edilen')
            axes[i].set_ylabel('GerÃ§ek')
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_roc_curves(self):
        """ROC eÄŸrileri"""
        plt.figure(figsize=(10, 8))
        
        colors = ['blue', 'green', 'red', 'orange']
        
        for i, (model_name, results) in enumerate(self.results.items()):
            fpr, tpr, _ = roc_curve(self.y_test, results['y_pred_proba'])
            auc_score = results['auc_score']
            
            plt.plot(fpr, tpr, color=colors[i], linewidth=2,
                    label=f'{model_name} (AUC = {auc_score:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC EÄŸrileri KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold', fontsize=14)
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'roc_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_feature_importance(self):
        """Feature importance (Random Forest iÃ§in)"""
        if 'Random Forest' not in self.models:
            return
        
        model = self.models['Random Forest']
        importances = model.feature_importances_
        feature_names = self.X_train.columns
        
        # En Ã¶nemli 20 Ã¶zelliÄŸi al
        indices = np.argsort(importances)[::-1][:20]
        
        plt.figure(figsize=(12, 8))
        plt.bar(range(20), importances[indices])
        plt.xticks(range(20), [feature_names[i] for i in indices], rotation=45, ha='right')
        plt.xlabel('Ã–zellikler')
        plt.ylabel('Ã–nem Skoru')
        plt.title('En Ã–nemli 20 Ã–zellik (Random Forest)', fontweight='bold', fontsize=14)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_class_distributions(self):
        """SÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # EÄŸitim seti daÄŸÄ±lÄ±mÄ±
        train_counts = pd.Series(self.y_train).value_counts()
        ax1.pie(train_counts.values, labels=['Normal', 'ZararlÄ±'], autopct='%1.1f%%',
               colors=['lightgreen', 'lightcoral'])
        ax1.set_title('EÄŸitim Seti SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±', fontweight='bold')
        
        # Test seti daÄŸÄ±lÄ±mÄ±
        test_counts = pd.Series(self.y_test).value_counts()
        ax2.pie(test_counts.values, labels=['Normal', 'ZararlÄ±'], autopct='%1.1f%%',
               colors=['lightgreen', 'lightcoral'])
        ax2.set_title('Test Seti SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'class_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_ml_report(self):
        """ML raporu oluÅŸtur"""
        print("ML raporu oluÅŸturuluyor...")
        
        # En iyi modeli bul
        best_model_name = max(self.results.keys(), 
                             key=lambda x: self.results[x]['auc_score'])
        best_results = self.results[best_model_name]
        
        # JSON raporu
        report_data = {
            "Mobil GÃ¼venlik ML Raporu": {
                "Tarih": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Veri Seti Bilgileri": {
                    "Toplam Ã¶rnek sayÄ±sÄ±": len(self.feature_data),
                    "Ã–zellik sayÄ±sÄ±": self.X_train.shape[1],
                    "EÄŸitim seti boyutu": len(self.X_train),
                    "Test seti boyutu": len(self.X_test)
                },
                "Model PerformanslarÄ±": {}
            }
        }
        
        # Her modelin sonuÃ§larÄ±nÄ± ekle
        for model_name, results in self.results.items():
            report_data["Mobil GÃ¼venlik ML Raporu"]["Model PerformanslarÄ±"][model_name] = {
                "Accuracy": results['accuracy'],
                "Precision": results['precision'],
                "Recall": results['recall'],
                "F1-Score": results['f1_score'],
                "AUC Score": results['auc_score'],
                "CV Mean": results['cv_mean'],
                "CV Std": results['cv_std']
            }
        
        report_data["Mobil GÃ¼venlik ML Raporu"]["En Ä°yi Model"] = {
            "Model AdÄ±": best_model_name,
            "AUC Score": best_results['auc_score'],
            "Accuracy": best_results['accuracy']
        }
        
        # JSON dosyasÄ±nÄ± kaydet
        with open(self.results_dir / 'ml_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
        
        # Metin raporu
        with open(self.results_dir / 'ml_report.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("MOBÄ°L GÃœVENLÄ°K VERÄ° SETÄ° - MAKÄ°NE Ã–ÄRENMESÄ° RAPORU\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"Rapor Tarihi: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("VERÄ° SETÄ° BÄ°LGÄ°LERÄ°:\n")
            f.write("-" * 20 + "\n")
            f.write(f"â€¢ Toplam Ã¶rnek sayÄ±sÄ±: {len(self.feature_data)}\n")
            f.write(f"â€¢ Ã–zellik sayÄ±sÄ±: {self.X_train.shape[1]}\n")
            f.write(f"â€¢ EÄŸitim seti boyutu: {len(self.X_train)}\n")
            f.write(f"â€¢ Test seti boyutu: {len(self.X_test)}\n\n")
            
            f.write("MODEL PERFORMANSLARI:\n")
            f.write("-" * 20 + "\n")
            for model_name, results in self.results.items():
                f.write(f"\n{model_name}:\n")
                f.write(f"  - Accuracy: {results['accuracy']:.4f}\n")
                f.write(f"  - Precision: {results['precision']:.4f}\n")
                f.write(f"  - Recall: {results['recall']:.4f}\n")
                f.write(f"  - F1-Score: {results['f1_score']:.4f}\n")
                f.write(f"  - AUC Score: {results['auc_score']:.4f}\n")
                f.write(f"  - CV Score: {results['cv_mean']:.4f} Â± {results['cv_std']:.4f}\n")
            
            f.write(f"\nEN Ä°YÄ° MODEL:\n")
            f.write("-" * 12 + "\n")
            f.write(f"â€¢ Model: {best_model_name}\n")
            f.write(f"â€¢ AUC Score: {best_results['auc_score']:.4f}\n")
            f.write(f"â€¢ Accuracy: {best_results['accuracy']:.4f}\n")
            
            f.write(f"\nOLUÅTURULAN DOSYALAR:\n")
            f.write("-" * 20 + "\n")
            f.write("â€¢ extracted_features.csv - Ã‡Ä±karÄ±lan Ã¶zellikler\n")
            f.write("â€¢ model_comparison.png - Model karÅŸÄ±laÅŸtÄ±rmasÄ±\n")
            f.write("â€¢ confusion_matrices.png - Confusion matrix'ler\n")
            f.write("â€¢ roc_curves.png - ROC eÄŸrileri\n")
            f.write("â€¢ feature_importance.png - Ã–zellik Ã¶nemi\n")
            f.write("â€¢ class_distributions.png - SÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±\n")
        
        # Modelleri kaydet
        for model_name, model in self.models.items():
            with open(self.results_dir / f'{model_name.lower().replace(" ", "_")}_model.pkl', 'wb') as f:
                pickle.dump(model, f)
        
        # Scaler'Ä± kaydet
        with open(self.results_dir / 'scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print("ML raporu oluÅŸturuldu!")
        print(f"En iyi model: {best_model_name} (AUC: {best_results['auc_score']:.4f})")
    
    def run_complete_pipeline(self, sample_size_per_class=300):
        """Tam ML pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r"""
        print("ğŸš€ Makine Ã–ÄŸrenmesi Pipeline baÅŸlatÄ±lÄ±yor...\n")
        
        # 1. Ã–zellik Ã§Ä±karÄ±mÄ±
        self.extract_all_features(sample_size_per_class)
        print()
        
        # 2. Veri hazÄ±rlama
        self.prepare_data_for_training()
        print()
        
        # 3. Model eÄŸitimi
        self.train_models()
        print()
        
        # 4. GÃ¶rselleÅŸtirmeler
        self.create_ml_visualizations()
        print()
        
        # 5. Rapor oluÅŸturma
        self.generate_ml_report()
        print()
        
        print("ğŸ‰ Makine Ã–ÄŸrenmesi Pipeline tamamlandÄ±!")
        print(f"SonuÃ§lar '{self.results_dir}' klasÃ¶rÃ¼nde saklandÄ±.")
        print("\nOluÅŸturulan dosyalar:")
        for file in sorted(self.results_dir.glob('*')):
            print(f"  â€¢ {file.name}")

if __name__ == "__main__":
    # ML Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
    ml_pipeline = MobileSecurityMLPipeline()
    ml_pipeline.run_complete_pipeline(sample_size_per_class=300) 